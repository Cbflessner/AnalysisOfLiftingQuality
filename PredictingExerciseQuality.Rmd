---
title: "Predicting Exercise Quality"
author: "Christian Flessner"
date: "December 05, 2018"
output: html_document
---

## Summary 
The report below suggests that one can very accurately assess the quatlity of exersise an individual engages in using data collected wearable fitness devices.

##Methodology 
Data was collected on 6 subjects who participated in activies in one of five way, one of which was correct as verified by a supervisor.  Data was collected on each of these activities via accelerometers on a belt, forearm, arm, and dumbell of each participant.

After downloading the training and test set the train set was first split into a training and validation set.  The training set was then further split into 10 folds in order to experiment with different algorithms.

```{r message=FALSE}
library(data.table)
library(caret)
library(dplyr)

train_val<-fread("pml-training.csv")
test<-fread("pml-testing.csv")
train_val$classe<-as.factor(train_val$classe)
intrain<-createDataPartition(y=train_val$classe, p=0.8, list=FALSE)
train<-train_val[intrain,]
val<-train_val[-intrain,]
kfold<-createFolds(y=train$classe, k=10, list=TRUE, returnTrain=FALSE)
```

The data was then filtered down to eliminate columns that contained no variability and columns that had missing values.  Due the the large amount of columns it was decided to simply exclude columns with missing values instead of imputing data.

```{r}
nsv<-nearZeroVar(train, saveMetrics = TRUE)
exc<-rownames(nsv[which(nsv$nzv),])
train_var<-train %>% select(-one_of(exc))
val_var<-val%>%select(-one_of(exc))

na_count <-sapply(train_var, function(y) sum(length(which(is.na(y)))))
na_count<-data.frame(rowname=names(na_count),count=na_count)
na_columns<- na_count %>% filter(count>0) %>% select(rowname)
na_columns<-as.vector(na_columns[,1])
train_na<-train_var %>% select(-one_of(na_columns))
val_na<-val_var %>% select(-one_of(na_columns))
```

Ultimately the gbm boosted decision tree algorithm was decided upon due to it's extreme accuracy in the 10 fold cross validation.

```{r cache=TRUE}
dt_test<-train_na[kfold[[1]]]
dt_train<-train_na[-kfold[[1]]]
dtree<-train(classe~., method="gbm", data=dt_train, verbose=FALSE)
pred<-predict(dtree,newdata = dt_test)
confusionMatrix(pred,dt_test$classe)$table
```

Building this model across all of the training data and applying it to the validation set got similiar results.

```{r cache=TRUE}
dt_model<-train(classe~., method="gbm", data=train_na, verbose=FALSE)
val_pred<-predict(dt_model, newdata=val_na)
confusionMatrix(val_pred, val_na$classe)$overall
```

Consequently, one can say with 95% certainty that the out of sample error rate will be 0.09% or below.
